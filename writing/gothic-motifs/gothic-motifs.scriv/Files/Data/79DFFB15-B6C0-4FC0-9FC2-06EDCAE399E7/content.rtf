{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 Georgia;}
{\colortbl;\red255\green255\blue255;\red252\green111\blue207;\red204\green102\blue255;}
{\*\expandedcolortbl;;\cssrgb\c100000\c54098\c84731;\cssrgb\c84466\c51457\c100000;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl312\slmult1\pardirnatural\partightenfactor0

\f0\fs22 \cf2 It is possible to computationally test the question of \'93can this information be used to classify these items according to these categories?\'94 [There are many binary classifiers which only need a \'93training set\'94 and a \'93testing set\'94 of data in order to evaluate their accuracy] Cross-validation allows us to use the same set of data for both training and testing. Five-fold cross-validation, for example, divides a dataset (in this case, a spreadsheet of all of Ann Tracy\'92s motifs and the books in which they appear) into five equal parts. The binary classifier \'93trains\'94 on the first four parts of the dataset, calculating which motifs correlate most strongly with male or female authorship. Then it \'93tests\'94 on the fifth part of the dataset, attempting to predict authorial gender for those works and then checking its prediction against the actual recorded gender in order to calculate the classifier\'92s accuracy. This process then repeats four more time, such that each of the five subsets of the dataset is used as the \'93test\'94 set [to reduce inaccuracy associated with sampling bias]. Using a classifier with a dataset asks the question: can this classifier use this data to make accurate predictions? A failure could indicate that the classifier is unsuited to the task, that the data insufficiently correlates to the categories, or that no difference exists between the two groups.\
I tried multiple binary classifiers with lacklustre results. The highest accuracy I got was {\field{\*\fldinst{HYPERLINK "scrivcmt://E6C54E5A-EA66-483E-8C53-EDBDD839EB80"}}{\fldrslt \cf2 66%,}} using only Frank\'92s tags. The highest I could achieve with Tracy was 59%.\
["what I can tell you is machines love data, and machines love it if you give them hints. 'oh? is sentimental a code word for female? I LOVE code words for female!'"]\
[This is why I talk about small sample sizes: Frank-Tracy-inner-join actually REDUCED accuracy, down to 60%, because it also reduced the size of the sample.] [It is tantalizingly tempting to draw conclusions anyway from small samples but we will be less accurate.]\
[What I learned from this attempt at binary classification is mostly that no, these aren\'92t very good ways at guessing authorial gender. The thing that comes closest is decoding the language of someone (Frank) who KNEW the gender when he wrote and could easily have been influenced by subconscious bias.] [Maybe different data would build a better classifier, \cf3 or maybe supervised machine learning (with a support vector machine) could classify better based on this data, \cf2 but frankly, on the strength of these results, I wouldn\'92t pursue one.]}