{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 Georgia;}
{\colortbl;\red255\green255\blue255;\red236\green184\blue255;\red206\green187\blue255;}
{\*\expandedcolortbl;;\cssrgb\c94500\c78400\c100000;\cssrgb\c84700\c79200\c100000;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl312\slmult1\pardirnatural\partightenfactor0

\f0\fs22 \cf0 It is possible to computationally test the question of \'93can this information be used to classify these items according to these categories?\'94 \cb2  [\'93Supervised\'94 machine learning uses  a \'93training set\'94 and a \'93testing set\'94 of data] [There are many machine learning \'93binary classifiers\'94 use existing classified data to infer a method for future classification]\cb1  Cross-validation allows us to use the same set of data for both training and testing. Five-fold cross-validation, for example, divides a dataset (such as a spreadsheet of all of Ann Tracy\'92s motifs and the books in which they appear) into five equal parts. The binary classifier \'93trains\'94 on the first four parts of the dataset, calculating which motifs correlate most strongly with male or female authorship. Then it \'93tests\'94 on the fifth part of the dataset, attempting to predict authorial gender for those works, and then checking its prediction against the actual recorded gender to calculate the classifier\'92s accuracy. This process then repeats four more times to reduce inaccuracy associated with sampling bias, by letting each of the five subsets of the dataset takes a turn as the \'93test\'94 set. Using a supervised machine learning binary classifier with a dataset asks the question: can this classifier use this data to make accurate predictions? A failure could indicate that the classifier is unsuited to the task, that the data insufficiently correlates to the categories, or that no difference exists between the two groups.\
\cb2 [I tried multiple binary classifiers with lacklustre results.]\cb1  The highest accuracy I got was {\field{\*\fldinst{HYPERLINK "scrivcmt://E6C54E5A-EA66-483E-8C53-EDBDD839EB80"}}{\fldrslt 66%,}} using only Frank\'92s tags. The highest I could achieve with Tracy was 59%.\cb2  [Not even worth running on the \'93unsigned\'94 works I have data for] [What I learned from this attempt at binary classification is mostly that no, these aren\'92t very good ways at guessing authorial gender. The thing that comes closest is decoding the language of someone (Frank) who KNEW the gender when he wrote and could easily have been influenced by subconscious bias.] [Maybe different data would build a better classifier, or maybe supervised machine learning (with a support vector machine) could classify better based on this data, but frankly, on the strength of these results, I wouldn\'92t pursue one.]\
\cb1 \
\cb3 TODO?? Experiment again, trying all 5 binary classifiers explained here: {\field{\*\fldinst{HYPERLINK "https://machinelearningmastery.com/use-classification-machine-learning-algorithms-weka/"}}{\fldrslt https://machinelearningmastery.com/use-classification-machine-learning-algorithms-weka/}}}