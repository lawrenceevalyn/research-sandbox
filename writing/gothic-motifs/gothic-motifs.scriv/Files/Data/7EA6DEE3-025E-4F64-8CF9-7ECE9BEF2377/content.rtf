{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 Georgia;}
{\colortbl;\red255\green255\blue255;\red236\green184\blue255;\red249\green198\blue219;}
{\*\expandedcolortbl;;\cssrgb\c94500\c78400\c100000;\cssrgb\c98400\c82400\c88600;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl312\slmult1\pardirnatural\partightenfactor0

\f0\fs22 \cf0 \cb2 [The other thing we learned from the classifier is that it is tantalizingly tempting to continue onward with someone else\'92s implicit bias.] [Frank was (marginally) more accurate because his \'93capta\'94 were captured in a way that was implicitly shaped by his knowledge of authorial gender \'97 which creates a classifier that will do better on its tests, but which can\'92t handle new information that is captured without Frank\'92s specific, individual, implicit bias.]\cb1  My computer scientist collaborator expressed \cb2 [this]\cb1  as \'93machines love data, and machines love it if you give them hints\'94. \cb2 [Other scholars have written a lot about this: machine learning mostly replicates the bias of the input. An algorithm to identify who to hire at Amazon, for example, will make its decisions based on who 
\i has been
\i0  hired, not who 
\i should
\i0  have been hired; because women are underrepresented in the pool of current employees, eliminating women from consideration will be an efficient way for the algorithm to zero on in the candidates with the highest certainty of being {\field{\*\fldinst{HYPERLINK "scrivcmt://52705FD1-1288-466D-99AE-7D0168E7CD2C"}}{\fldrslt \cb2 hired.}} Rather than \'91objectively\'92 eliminating hiring bias, therefore, it reinforces it. Even if it is not provided direct data regarding gender, it will find ways to infer biased data creation: if female names are manually removed, it will zero in on the names of all-women\'92s colleges; if those are removed, it will zero in on service organizations for women.][In the case of this project,]\cb1  if \'93sentimental\'94 is\cb2  [a female-coded term / code word for \'93female\'94], \cb1 the classifier will \cb2 [seize/expand/extrapolate from] \cb1 that implicit bias and perpetuate it. Revisiting Frank\'92s terminology with an eye for these coded shades of difference the interpretive multiplicity which earlier \cb2 [seemed like a critical strength] \cb1 now seem like a barrier to\cb2  [ongoing/extensible study]. [What makes one work \'93polemical\'94 and another \'93didactic\'94, other than the fact that men are much more likely to be \'93polemical\'94?] [TODO: several more examples of Frank\'92s gendered terminology]  \cb1 His \'93capta\'94 are captured too \cb2 implicitly/subjectively\cb1 , so they are best able to tell us \'93did Frank think this author\'92s gender played a role in their decisions when he described their genre?\'94\
\
\cb3 \'93It penalized resumes that included the word \'91women\'92s,\'92 as in \'91women\'92s chess club captain.\'92 And it downgraded graduates of two all-women\'92s colleges\'94 (Dastin)\
\'93Amazon edited the programs to make them neutral to these particular terms. But that was no guarantee that the machines would not devise other ways of sorting candidates that could prove discriminatory\'94 (Dastin)}